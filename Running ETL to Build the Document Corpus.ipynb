{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ETL to Build the Document Corpus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process for setting up the corpus of Full Stack documents that the bot searches over.\n",
    "\n",
    "In each case, we have to\n",
    "\n",
    "- Extract data from its natural habitat, like YouTube or GitHub\n",
    "- Transform it into a format that is useful for our purposes\n",
    "- Load it into our database in that format\n",
    "\n",
    "hence the acronym \"ETL\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ENV=dev\n",
    "!make secrets  # you'll need credentials for Mongo and Modal to run this\n",
    "db, collection = \"fsdl-dev\", \"ask-fsdl\"  # we run this in a dev database\n",
    "\n",
    "# drop the collection if it exists, it's just dev\n",
    "!modal run app.main::drop_docs --db {db} --collection {collection}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "from etl.shared import display_modal_image\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs: arXiV Papers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!modal run etl/pdfs.py --json-path data/llm-papers.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(shared.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(pdfs.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_path = Path(\"data\") / \"llm-papers.json\"\n",
    "\n",
    "with open(papers_path) as f:\n",
    "    pdf_infos = json.load(f)\n",
    "\n",
    "pdf_infos[:100:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfs.stub.run():\n",
    "    # first, we enrich the paper data by finding direct PDF URLs where we can\n",
    "    paper_data = pdfs.get_pdf_url.map(\n",
    "        pdf_infos[::25], return_exceptions=True  # subsampling to run faster\n",
    "    )\n",
    "    # then we turn the PDFs into JSON documents\n",
    "    documents = shared.unchunk(  # each pdf creates a list of documents, one per page, so we flatten\n",
    "        # after we run the extract_pdf function on Modal to get those pages\n",
    "        pdfs.extract_pdf.map(paper_data, return_exceptions=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(documents[0][\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    # we split our document list into 10 pieces, so that we don't open too many connections\n",
    "    chunked_documents = shared.chunk_into(documents, 10)\n",
    "    list(\n",
    "        shared.add_to_document_db.map(\n",
    "            chunked_documents, kwargs={\"db\": db, \"collection\": collection}\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    # pull only arxiv papers\n",
    "    query = {\"metadata.source\": {\"$regex\": \"arxiv\\.org\", \"$options\": \"i\"}}\n",
    "    # project out the text field, it can get large\n",
    "    projection = {\"text\": 0}\n",
    "    # get just one result to show it worked\n",
    "    result = shared.query_one.remote(query, projection, db=db, collection=collection)\n",
    "\n",
    "pp.pprint(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown Files: Lectures\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!modal run etl/markdown.py --json-path data/lectures-2022.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(markdown.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_path = Path(\"data\") / \"lectures-2022.json\"\n",
    "\n",
    "with open(markdown_path) as f:\n",
    "    markdown_corpus = json.load(f)\n",
    "\n",
    "website_url, md_url = (\n",
    "    markdown_corpus[\"website_url_base\"],\n",
    "    markdown_corpus[\"md_url_base\"],\n",
    ")\n",
    "\n",
    "lectures = markdown_corpus[\"lectures\"]\n",
    "\n",
    "lectures[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with markdown.stub.run():\n",
    "    documents = shared.unchunk(  # each lecture creates multiple documents, one per section so we flatten\n",
    "        markdown.to_documents.map(\n",
    "            lectures,\n",
    "            kwargs={\"website_url\": website_url, \"md_url\": md_url},\n",
    "            return_exceptions=True,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(documents[1][\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=documents[1][\"metadata\"][\"source\"], width=800, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    chunked_documents = shared.chunk_into(documents, 10)\n",
    "    list(\n",
    "        shared.add_to_document_db.map(\n",
    "            chunked_documents, kwargs={\"db\": db, \"collection\": collection}\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    # pull only lectures\n",
    "    query = {\"metadata.source\": {\"$regex\": \"lecture\", \"$options\": \"i\"}}\n",
    "    # project out the text field, it can get large\n",
    "    projection = {\"text\": 0}\n",
    "    # get just one result to show it worked\n",
    "    result = shared.query_one.remote(query, projection, db=db, collection=collection)\n",
    "\n",
    "pp.pprint(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos: YouTube Transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(videos.image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_path = Path(\"data\") / \"videos.json\"\n",
    "\n",
    "with open(videos_path) as f:\n",
    "    video_infos = json.load(f)\n",
    "\n",
    "video_ids = [video[\"id\"] for video in video_infos]\n",
    "\n",
    "video_infos[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with videos.stub.run():\n",
    "    documents = shared.unchunk(  # each lecture creates multiple documents, one per chapter, so we flatten\n",
    "        videos.extract_subtitles.map(\n",
    "            video_infos[-3:],  # subsampling to run faster\n",
    "            return_exceptions=True,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(documents[1][\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "id_str, time_str = documents[1][\"metadata\"][\"source\"].split(\"?v=\")[-1].split(\"&t=\")\n",
    "YouTubeVideo(id_str, start=int(time_str.strip(\"s\")), width=800, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    chunked_documents = shared.chunk_into(documents, 10)\n",
    "    list(\n",
    "        shared.add_to_document_db.map(\n",
    "            chunked_documents, kwargs={\"db\": db, \"collection\": collection}\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    # pull only lectures\n",
    "    query = {\"metadata.source\": {\"$regex\": \"youtube\", \"$options\": \"i\"}}\n",
    "    # project out the text field, it can get large\n",
    "    projection = {\"text\": 0}\n",
    "    # get just one result to show it worked\n",
    "    result = shared.query_one.remote(query, projection, db=db, collection=collection)\n",
    "\n",
    "pp.pprint(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ask-fsdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
